---
title: 'ST 551: Statistical Methods I'
date: "Lab 08 - Distribution tests"
output: html_notebook
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```


# The Kolmogorov-Smirnov Test

You can perform the Kolmogorov-Smirnov (KS) test in R by using the `ks.test()` function. For the function to work, you provide a data set along with a hypothesized distribution function (like `pnorm`, `punif`, `pchisq`, or anything else with a `p____`) and a set of hypothesized parameters for the distribution. To see how it works, lets first generate some data:

```{r}
# Fix a sample size
n <- 20

# Now let's generate some data from a standard normal distribution
obs <- rnorm(n = n, mean = 0, sd = 1)
```

Now let's see what the results of the KS test look like for different proposed distributions:

```{r}
# KS test for the true probability distribution
ks.test(obs, pnorm, 0, 1)
```

Notice here that we fail to reject the null hypothesis that $H_0: F(x) = \textrm{Normal}(0,1)$. This indicates then that the true probability distribution is a plausible distribution which generated the data. Notice however that there are other proposed distributions that the KS test also finds plausible:

```{r}
ks.test(obs, pt, 0.1, 0.8)
```

Here again, even with the incorrect parameterization, the KS test indicates that this untrue probability distribution is also a plausible distribution which generated the data. 

### Question 1: Based on your sample, find another probability distribution (other than the Normal distribution) which the KS test also finds to plausibly generate the data.

```{r}
# Run your KS test here

```

Finally, you might be interested in making a plot similar to what we showed in the lecture where you compare the ecdf function of the observations to the proposed distribution from you null hypothesis. One, not terribly difficult, way to do this is by using `ggplot2`

```{r}
# Load the ggplot2 library
library(ggplot2)

# Initialize your ggplot
ggplot() + 
  
  # Add a computed ecdf of your observations
  stat_ecdf(aes(x = obs)) +
  
  # Add a probability distribution
  stat_function(fun = pnorm, args = c(0, 1), linetype = "dashed") +
  
  # Change the theme to minimal since it's class
  theme_minimal()
```


# The Pearson Chi-squared Test

Comparing observations to discrete probability distributions is made possible by the `chisq.test()` function. But here, the process is slightly more involved because (1) We need to tabulate how many times each of our discrete values occurred and (2) we need to specify _all values which are possible_ and not just those that were _actually observed_.

To begin then, we'll start by generating a new sample of discrete integers between 1 and 10:

```{r}
discrete_obs <- sample(1:10, size = 50, replace = TRUE)
```

Next, we use the `table()` function to count the number of occurrences which occurred in each "category" (really just discrete integer values)

```{r}
tab_obs <- table(discrete_obs)
```

Once we've tabulated our values, we can use the `chisq.test()` function's default settings to see if there's reason to believe the values are not uniform random (i.e. each value has an equal probability of occurring):

```{r}
chisq.test(tab_obs)
```

Notice here that, by and large, we should fail to reject the null hypothesis since our data are indeed discrete uniform samples. What the Chi-squared test is indicating then is that a discrete uniform probability mass function is a plausible distribution which generated the data. However, like the KS test, this distribution isn't necessarily the only plausible distribution which might have generated the data.

While testing if the observed counts are discrete uniform is a useful default, it's far from the only distribution we might wish to test with the `chisq.test()` function. For instance, suppose we wanted to see use the `chisq.test()` function to determine if the values might plausibly have been generated by the following pmf:
\[ p(x) = 
\begin{cases}
  \dfrac{x}{\sum_{i = 1}^{10} x_i}, x\in \{1, 2, \ldots, 10\} \\
  0, \text{otherwise}
\end{cases}
\]

This mass function is clearly non-uniform, so to compare our observations with the `chisq.test()` function, we need to specify the `p` argument with the probabilities:

```{r}
# Generate the probs
probs <- (1:10) / sum(1:10)

# Ensure that this is in fact a valid pmf:
sum(probs)

# Conduct a Chi-squared test
chisq.test(tab_obs, p = probs)
```

One thing that may happen when you run the Chi-squared test is that R might return a warning stating: `Chi-squared approximation may be incorrect`. This occurs when R detects that the computed p-value might be incorrect since the conditions for the chi-squared approximation might not be valid. We can overcome this warning however by using the `simulate.p.value` argument (Feel free to read the help docs to see the exact reason why this warning appears):

```{r}
chisq.test(tab_obs, p = probs, simulate.p.value = TRUE)
```

Here, we reject our null hypothesis that our observations came from the pmf function above. Meaning that it's unlikely that the pmf we specified actually generated our observations. 

## Missing values?

One issue you might face when conducting a chi-squared test in R is that the values that we observe (and count with the `table()` function) may not represent all the values which are _possible_. To see what we mean, try the following (I use `set.seed()` here so that you can reproduce the same random results that I'm generating):

```{r}
set.seed(1511)

# Generate a new sample but this time use the pmf function from above
new_samp <- sample(1:10, size = 50, replace = TRUE, prob = probs)
```

If we now look at the table of counts, we should notice that the value of `1` while possible, didn't actually occur and thus is "missing":

```{r}
table(new_samp)
```

That is, if `1` doesn't occur in the sample than it's not tabulated in `table()`. But, the value of 1 is possible with $p(1) = 1 / 55$. Hence, we need to do _something_ which will count all possible categories (according to our hypothesis) and not just those that are observed.

Like many things in R, there's a variety of ways to accomplish this task (I actually discovered a new one on my own when writing this lab using the `tabulate()` function, but I digress). For now, I'll show you two approaches.

The first approach involves converting our data to "factors" (R speak for "categorical variables") and to specify all of the possible levels of the factor.

```{r}
# Convert the sample to a factor and specify the possible values
# as "levels"
new_samp_f <- factor(new_samp, levels = 1:10)
table(new_samp_f)
```

The other approach is to (1) augment the data with all the possible values of the variable, (2) count the resulting output, and then (3) subtract 1 from each count to preserve the zeros:

```{r}
# (1) Augment the data with all possible vlues:
new_samp_aug <- c(new_samp, 1:10)

# (2) Count the velues
tab_aug <- table(new_samp_aug)
tab_aug

# (3) Subtract 1 from each count
tab_aug - 1
```

### Question 2: Use the `chisq.test()` function to see if `new_samp` was plausibly generted by the pmf described by `probs`.

```{r}
# Answer the question below

```

## Estimating parameters

Finally, let's consider the case when we want to estimate some parameter values for our null hypothesis (which again is a feature that's not possible for the KS test). First, let's generate data from a Poisson(4) distribution and then estimate the value of the parameter, `lambda`, using the sample mean:

```{r}
n <- 50
pois_samp <- rpois(n = n, lambda = 4)
lam_hat <- mean(pois_samp)
table(pois_samp)
```

Since the Poisson distribution can be _any_ non-negative integer, we need to truncate our table to that there will be $k$ values, i.e. 0, 1, 2, ..., k-1. This is necessary since we can't specify an infinite number of categories (plus, we'd rther not have categories with very small expected counts).

To compute the truncated table, we use the augmented approach to make sure that the categories with 0 counts are still represented. Then, we make a table for all of the values less than k-1 and concatenate the table with a cound of how many are greater than or equal to k-1:

```{r}
# Truncate the table so that there will be k "categories"
k <- 5 
pois_samp_aug <- c(pois_samp, 0:(k-1))

# Tabulate the augmented values and add sum the values that 
# are greated than k-1
tab_pois_aug <- c(table(pois_samp_aug[pois_samp_aug < (k-1)]),
                  sum(pois_samp_aug >= (k-1))) - 1
tab_pois_aug
```

The expected counts are computed using the Poisson probability mass function `dpois()` with parameter `lambda` equal to the estimated value `lam_hat`. The expected count for the final category is the $n$ times the probability of a Poisson random variable is greater than or equal to $k-1$.

```{r}
tab_expect <- c(dpois(0:(k-2), lambda = lam_hat),
                1 - ppois(k-2, lambda = lam_hat)) * n
tab_expect
```

We can view our observed counts (`tab_pois_aug`) with our expected counts (`tab_expect`) now as follows:

```{r}
rbind(tab_pois_aug, tab_expect)
```

To compute the Pearson's chi-squared test statistic by hand, we compute:

```{r}
chisq_stat <- sum((tab_pois_aug - tab_expect)^2 / tab_expect)
chisq_stat
```

And the p-value then comes from a chi-squared distribution with $k-2$ degrees of freedom (We "spend" one degree of freedom estimating the parameter value and another on the constraint on the total sum of the expected counts)

```{r}
p_val <- 1 - pchisq(chisq_stat, df = k - 2)
p_val
```

Since we fail to reject our null hypothesis, it's plausible that our data came from a truncated Poisson probability distribution with df = `lam_hat` (Your value of `lam_hat` may vary, and I guess now that I think of it your p-value might vary as well... if you don't fail to reject, just run the code again and you (very likely) will generate a result similar to what I generate).

We can perform the same test we did by hand using the `chisq.test()` function as well:

```{r}
chisq.test(tab_pois_aug, p = tab_expect / n)
```

Note here that the test statistic should agree with the value you computed but the p-value may not. This is because the `chisq.test()` function doesn't know that you've spent a degree of freedom estimating the parameter value! 

### Question 3: Compute the p-value of your computed test statistic using a chi-squared distribution with $k-1$ degrees of freedom and note if it agrees with the p-value from the `chisq.test()` function. Remember though, this isn't technically correct since you lost a degree of freedom by estimating the value of `lambda` with `lam_hat`.

```{r}
# Write your answer here

```

Since the `chisq.test()` function does not have an easy option for estimating parameters, and does not use the appropriate degrees of freedom when your expected counts are based on estimated parameter, we can simulate to confirm that we get a better calibrated (more exact) test when using k-2 degrees of freedom instead of k-1

```{r}
n_sim <- 10000
n <- 50

# Create two storage vectors for our simulations
p_vals_k_minus_2 <- vector(length = n_sim)
p_vals_k_minus_1 <- vector(length = n_sim)

# Simulate!

for (i in 1:n_sim) {
  # Generate a random sample
  new_samp <- rpois(n, lambda = 4)
  
  # Estimate the parameter
  lam_hat <- mean(new_samp)
  
  # Choose k = 5 categories
  new_samp_aug <- c(new_samp, 0:(k-1))
  
  # Compute the observed counts
  tab_obs <- c(table(new_samp_aug[new_samp_aug < (k-1)]),
               sum(new_samp_aug >= (k-1))) - 1
  
  # Compute the expected counts
  tab_exp <- c(dpois(0:(k-2), lambda = lam_hat),
               1 - ppois(k-2, lambda = lam_hat)) * n
  
  # Compute the chi-squared test stat
  chisq_stat <- sum((tab_obs - tab_exp)^2 / tab_exp)
  
  # Compute the p-value using k-2 degrees of freedom (Correct)
  p_vals_k_minus_2 <- 1 - pchisq(chisq_stat, df = k - 2)
  
  # Compute the p-values using k-1 degrees of freedom (Wrong)
  p_vals_k_minus_1 <- chisq.test(tab_obs, p = tab_exp / n)$p.val
}

# Compute the proportion of the simulated p-values that are 
# rejected at the alpha = 0.05 significance level
mean(p_vals_k_minus_2)
mean(p_vals_k_minus_1)
```

### Question 4: Which of the proportions of the simulated p-values that are rejected at the alpha = 0.05 significance level is better? Note: The null hypothesis is true in that our data do indeed come from a Poisson distribution

<!-- Write your answer here -->